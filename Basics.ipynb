{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as tc      \n",
    "tc.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor and Gradient\n",
    "## Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n",
      "torch.int64\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "torch.int64\n",
      "torch.Size([5])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = tc.tensor(4)\n",
    "print(t1)\n",
    "print(t1.dtype)\n",
    "\n",
    "t2 = tc.tensor([1,2,3,4,5])\n",
    "print(t2)\n",
    "print(t2.dtype)\n",
    "print(t2.shape)\n",
    "\n",
    "mat = tc.tensor([[1,2],[3,4]])\n",
    "print(mat)\n",
    "\n",
    "twothree = tc.tensor([[1,2,3],[4,5,6]])\n",
    "print(twothree)\n",
    "print(twothree.shape)\n",
    "print(twothree.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17., grad_fn=<AddBackward0>)\n",
      "dy/dx  None\n",
      "dy/dw  tensor(3.)\n",
      "dy/db  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# These tensors are just scalar\n",
    "x = tc.tensor(3)  # Only to the tensors that we really needs the gradient\n",
    "w = tc.tensor(4., requires_grad=True)\n",
    "b = tc.tensor(5., requires_grad=True)\n",
    "\n",
    "# Arithmetic operation\n",
    "y = w*x + b\n",
    "print(y)\n",
    "\n",
    "# Calculate gradients\n",
    "y.backward() # Call this to trigger grad calculation\n",
    "print('dy/dx ', x.grad) \n",
    "print('dy/dw ', w.grad) #.grad attributes is wrt to the y.backward()\n",
    "print('dy/db ', b.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "int64\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.int64\n",
      "[[0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2],[3,4]])\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "y = tc.from_numpy(x)\n",
    "print(y)\n",
    "print(y.dtype)\n",
    "z = y.numpy()\n",
    "print(x - z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "w\n",
      "tensor([[-2.7558,  1.4785],\n",
      "        [-0.1469, -0.3173],\n",
      "        [-0.0935, -0.7446]], requires_grad=True)\n",
      "b\n",
      "tensor([ 0.2452, -0.3965], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "                   # Targets (apples, oranges)\n",
    "output = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "input_tc = tc.from_numpy(input)\n",
    "output_tc = tc.from_numpy(output)\n",
    "print(input_tc)\n",
    "print(output_tc)\n",
    "\n",
    "# y = w*x + b\n",
    "w = tc.tensor([[-2.7558,  1.4785],\n",
    "        [-0.1469, -0.3173],\n",
    "        [-0.0935, -0.7446]], requires_grad = True)\n",
    "b = tc.tensor([ 0.2452, -0.3965], requires_grad = True)\n",
    "print(\"w\")\n",
    "print(w)\n",
    "print(\"b\")\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(x): \n",
    "    return x @ w + b\n",
    "def mse(t1,t2):\n",
    "    diff = (t1 - t2)\n",
    "    return tc.sum((diff*diff)/diff.numel()) # numel = number of element\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-214.7910,   54.2571],\n",
      "        [-269.4438,   58.5702],\n",
      "        [-264.6170,   42.5280],\n",
      "        [-290.6226,  109.2164],\n",
      "        [-210.5524,   19.0372]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor(56478.9727, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "preds = model(input_tc)\n",
    "print(preds)\n",
    "print(output_tc)\n",
    "\n",
    "loss = mse (preds, output_tc)\n",
    "print(loss)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7558,  1.4785],\n",
      "        [-0.1469, -0.3173],\n",
      "        [-0.0935, -0.7446]], requires_grad=True)\n",
      "tensor([[-27711.0879,  -2482.5532],\n",
      "        [-28786.1074,  -4680.5933],\n",
      "        [-17967.5820,  -2593.0432]])\n",
      "tensor([ 0.2452, -0.3965], requires_grad=True)\n",
      "tensor([-326.2054,  -35.2782])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient of loss wrt to the components\n",
    "print(w)\n",
    "print(w.grad)\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 1e-5\n",
    "with tc.no_grad():\n",
    "    w -= w.grad*rate\n",
    "    b -= b.grad*rate\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_over_time = []\n",
    "for i in range(100):\n",
    "    preds = model(input_tc)\n",
    "    loss = mse(preds, output_tc)\n",
    "    loss_over_time.append(loss.detach().numpy())\n",
    "    loss.backward()\n",
    "    with tc.no_grad(): # Turn off the gradient feature to reduce memory while update\n",
    "        w -= w.grad*rate\n",
    "        b -= b.grad*rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf8c820760>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,len(loss_over_time),num = len(loss_over_time))\n",
    "\n",
    "plt.plot(x, loss_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([73., 67., 43.]), tensor([56., 70.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(input_tc, output_tc) # Input Output\n",
    "print(type(train_ds))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "tensor([[ 69.,  96.,  70.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[103., 119.],\n",
      "        [119., 133.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [ 22.,  37.]])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "train_dataloader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\n",
    "for xb, yb in train_dataloader:\n",
    "    print(\"batch\")\n",
    "    print(xb)\n",
    "    print(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2585, -0.4612, -0.1896],\n",
      "        [-0.0336, -0.4540,  0.3737]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3565,  0.1285], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2585, -0.4612, -0.1896],\n",
       "         [-0.0336, -0.4540,  0.3737]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3565,  0.1285], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-58.2846, -16.6769],\n",
       "        [-76.6059, -18.9686],\n",
       "        [-95.6497, -41.9625],\n",
       "        [-53.5748,  -8.9980],\n",
       "        [-75.7460, -19.6184]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(input_tc)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19548.4883, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_fn = F.mse_loss\n",
    "loss = loss_fn(model(input_tc), output_tc)\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mrequired\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Implements stochastic gradient descent (optionally with momentum).\n",
      "\n",
      ".. math::\n",
      "   \\begin{aligned}\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      "            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      "        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
      "        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      "        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      "        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      "        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      "        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      "        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
      "        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n",
      "        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      "        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
      "        &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "   \\end{aligned}\n",
      "\n",
      "Nesterov momentum is based on the formula from\n",
      "`On the importance of initialization and momentum in deep learning`__.\n",
      "\n",
      "Args:\n",
      "    params (iterable): iterable of parameters to optimize or dicts defining\n",
      "        parameter groups\n",
      "    lr (float): learning rate\n",
      "    momentum (float, optional): momentum factor (default: 0)\n",
      "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      "    dampening (float, optional): dampening for momentum (default: 0)\n",
      "    nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      "    maximize (bool, optional): maximize the params based on the objective, instead of\n",
      "        minimizing (default: False)\n",
      "    foreach (bool, optional): whether foreach implementation of optimizer\n",
      "        is used (default: None)\n",
      "\n",
      "Example:\n",
      "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "    >>> optimizer.zero_grad()\n",
      "    >>> loss_fn(model(input), target).backward()\n",
      "    >>> optimizer.step()\n",
      "\n",
      "__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      "\n",
      ".. note::\n",
      "    The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      "    Sutskever et. al. and implementations in some other frameworks.\n",
      "\n",
      "    Considering the specific case of Momentum, the update can be written as\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      "            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      "        \\end{aligned}\n",
      "\n",
      "    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      "    parameters, gradient, velocity, and momentum respectively.\n",
      "\n",
      "    This is in contrast to Sutskever et. al. and\n",
      "    other frameworks which employ an update of the form\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      "            p_{t+1} & = p_{t} - v_{t+1}.\n",
      "        \\end{aligned}\n",
      "\n",
      "    The Nesterov version is analogously modified.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/learn/lib/python3.8/site-packages/torch/optim/sgd.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     NewCls\n"
     ]
    }
   ],
   "source": [
    "optimize = tc.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "?tc.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(EPOCH, model, loss_fn, optimize):\n",
    "    for epoch in range(EPOCH):\n",
    "        for xb, yb in train_dataloader:\n",
    "            loss = loss_fn(model(xb), yb) # The function that we want the error to be the lowest\n",
    "            loss.backward()\n",
    "            optimize.step() # Update the parameter using the gradient SGD\n",
    "            optimize.zero_grad() # Reset the grad of the parameter\n",
    "        if (epoch+1)% 10 ==0:\n",
    "            print(\"Epoch [{}/{}], Loss: {:.4f}\".format(epoch+1,EPOCH,loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 1163.1074\n",
      "Epoch [20/1000], Loss: 559.6546\n",
      "Epoch [30/1000], Loss: 484.5746\n",
      "Epoch [40/1000], Loss: 427.2568\n",
      "Epoch [50/1000], Loss: 376.9379\n",
      "Epoch [60/1000], Loss: 332.6158\n",
      "Epoch [70/1000], Loss: 293.5718\n",
      "Epoch [80/1000], Loss: 259.1765\n",
      "Epoch [90/1000], Loss: 228.8750\n",
      "Epoch [100/1000], Loss: 202.1792\n",
      "Epoch [110/1000], Loss: 178.6587\n",
      "Epoch [120/1000], Loss: 157.9351\n",
      "Epoch [130/1000], Loss: 139.6746\n",
      "Epoch [140/1000], Loss: 123.5836\n",
      "Epoch [150/1000], Loss: 109.4031\n",
      "Epoch [160/1000], Loss: 96.9056\n",
      "Epoch [170/1000], Loss: 85.8903\n",
      "Epoch [180/1000], Loss: 76.1806\n",
      "Epoch [190/1000], Loss: 67.6206\n",
      "Epoch [200/1000], Loss: 60.0735\n",
      "Epoch [210/1000], Loss: 53.4184\n",
      "Epoch [220/1000], Loss: 47.5493\n",
      "Epoch [230/1000], Loss: 42.3722\n",
      "Epoch [240/1000], Loss: 37.8049\n",
      "Epoch [250/1000], Loss: 33.7748\n",
      "Epoch [260/1000], Loss: 30.2178\n",
      "Epoch [270/1000], Loss: 27.0776\n",
      "Epoch [280/1000], Loss: 24.3047\n",
      "Epoch [290/1000], Loss: 21.8554\n",
      "Epoch [300/1000], Loss: 19.6911\n",
      "Epoch [310/1000], Loss: 17.7780\n",
      "Epoch [320/1000], Loss: 16.0863\n",
      "Epoch [330/1000], Loss: 14.5896\n",
      "Epoch [340/1000], Loss: 13.2649\n",
      "Epoch [350/1000], Loss: 12.0916\n",
      "Epoch [360/1000], Loss: 11.0520\n",
      "Epoch [370/1000], Loss: 10.1300\n",
      "Epoch [380/1000], Loss: 9.3119\n",
      "Epoch [390/1000], Loss: 8.5853\n",
      "Epoch [400/1000], Loss: 7.9394\n",
      "Epoch [410/1000], Loss: 7.3648\n",
      "Epoch [420/1000], Loss: 6.8529\n",
      "Epoch [430/1000], Loss: 6.3965\n",
      "Epoch [440/1000], Loss: 5.9889\n",
      "Epoch [450/1000], Loss: 5.6246\n",
      "Epoch [460/1000], Loss: 5.2984\n",
      "Epoch [470/1000], Loss: 5.0058\n",
      "Epoch [480/1000], Loss: 4.7431\n",
      "Epoch [490/1000], Loss: 4.5066\n",
      "Epoch [500/1000], Loss: 4.2933\n",
      "Epoch [510/1000], Loss: 4.1007\n",
      "Epoch [520/1000], Loss: 3.9262\n",
      "Epoch [530/1000], Loss: 3.7679\n",
      "Epoch [540/1000], Loss: 3.6239\n",
      "Epoch [550/1000], Loss: 3.4925\n",
      "Epoch [560/1000], Loss: 3.3724\n",
      "Epoch [570/1000], Loss: 3.2622\n",
      "Epoch [580/1000], Loss: 3.1609\n",
      "Epoch [590/1000], Loss: 3.0675\n",
      "Epoch [600/1000], Loss: 2.9811\n",
      "Epoch [610/1000], Loss: 2.9010\n",
      "Epoch [620/1000], Loss: 2.8265\n",
      "Epoch [630/1000], Loss: 2.7570\n",
      "Epoch [640/1000], Loss: 2.6919\n",
      "Epoch [650/1000], Loss: 2.6309\n",
      "Epoch [660/1000], Loss: 2.5734\n",
      "Epoch [670/1000], Loss: 2.5192\n",
      "Epoch [680/1000], Loss: 2.4680\n",
      "Epoch [690/1000], Loss: 2.4193\n",
      "Epoch [700/1000], Loss: 2.3731\n",
      "Epoch [710/1000], Loss: 2.3290\n",
      "Epoch [720/1000], Loss: 2.2869\n",
      "Epoch [730/1000], Loss: 2.2465\n",
      "Epoch [740/1000], Loss: 2.2078\n",
      "Epoch [750/1000], Loss: 2.1707\n",
      "Epoch [760/1000], Loss: 2.1349\n",
      "Epoch [770/1000], Loss: 2.1003\n",
      "Epoch [780/1000], Loss: 2.0670\n",
      "Epoch [790/1000], Loss: 2.0347\n",
      "Epoch [800/1000], Loss: 2.0035\n",
      "Epoch [810/1000], Loss: 1.9732\n",
      "Epoch [820/1000], Loss: 1.9438\n",
      "Epoch [830/1000], Loss: 1.9152\n",
      "Epoch [840/1000], Loss: 1.8874\n",
      "Epoch [850/1000], Loss: 1.8603\n",
      "Epoch [860/1000], Loss: 1.8339\n",
      "Epoch [870/1000], Loss: 1.8082\n",
      "Epoch [880/1000], Loss: 1.7831\n",
      "Epoch [890/1000], Loss: 1.7586\n",
      "Epoch [900/1000], Loss: 1.7347\n",
      "Epoch [910/1000], Loss: 1.7113\n",
      "Epoch [920/1000], Loss: 1.6885\n",
      "Epoch [930/1000], Loss: 1.6661\n",
      "Epoch [940/1000], Loss: 1.6442\n",
      "Epoch [950/1000], Loss: 1.6228\n",
      "Epoch [960/1000], Loss: 1.6019\n",
      "Epoch [970/1000], Loss: 1.5814\n",
      "Epoch [980/1000], Loss: 1.5613\n",
      "Epoch [990/1000], Loss: 1.5416\n",
      "Epoch [1000/1000], Loss: 1.5223\n"
     ]
    }
   ],
   "source": [
    "fit(1000,model, loss_fn, optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transform # Transform other data types into tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MNIST(root='data/', download=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root=\"data/\", train = False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'int'>\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "image, label = dataset[0]\n",
    "print(type(dataset[0]))\n",
    "print(type(image))\n",
    "print(type(label))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667,\n",
      "         0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root=\"data/\", train=True, transform = transform.ToTensor()) # This is only applicable to MNIST\n",
    "\n",
    "# Transform to  C x H x W\n",
    "\n",
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)\n",
    "print(img_tensor[0][10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Validate\n",
    "from sklearn. model_selection import train_test_split\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CustomModule(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.con1 = nn.Conv2d(2, 2, 3)\\n        self.con2 = nn.Conv2d(2, 3, 3)\\n\\n    def forward(self, X):\\n        x = self.con1(x)\\n        x = self.con2(x)\\n        return X\\n\\nmodel = CustomModule(input, output)\\nopt = tc.optim.SGD(model.parameters(), lr = 0.1) # the model is a child class of nn.Module, hence, it has method parameters()\\nF.mse_loss\\n\\n# Training process\\n\\nfor epoch in range(EPOCH_NUM):\\n    predict = model.forward(input)\\n    loss = F.mse_loss(predict, output)\\n    loss.backward()\\n    opt.step()\\n    opt.zero_grad()\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class CustomModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.con1 = nn.Conv2d(2, 2, 3)\n",
    "        self.con2 = nn.Conv2d(2, 3, 3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.con1(x)\n",
    "        x = self.con2(x)\n",
    "        return X\n",
    "\n",
    "model = CustomModule(input, output)\n",
    "opt = tc.optim.SGD(model.parameters(), lr = 0.1) # the model is a child class of nn.Module, hence, it has method parameters()\n",
    "F.mse_loss\n",
    "\n",
    "# Training process\n",
    "\n",
    "for epoch in range(EPOCH_NUM): # Run epoch\n",
    "    for input,output in DataLoad: # Update after 1 batch\n",
    "        predict = model.forward(input)\n",
    "        loss = F.mse_loss(predict, output)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data\n",
    "breastCancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data\n",
    "X, y = breastCancer.data, breastCancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n",
    "# Scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = tc.from_numpy(X_train.astype(np.float32))\n",
    "X_test = tc.from_numpy(X_test.astype(np.float32))\n",
    "y_train = tc.from_numpy(y_train.astype(np.float32))\n",
    "y_test = tc.from_numpy(y_test.astype(np.float32))\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = CustomModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0874, 0.1001, 0.0985, 0.1023, 0.1038, 0.0992, 0.0872, 0.1136, 0.0991,\n",
      "         0.1090]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1,28,28,device = device)\n",
    "logits = model(X)\n",
    "pred_prob = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_prob.argmax(1)\n",
    "print(pred_prob)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wrapper(original):\n",
    "#     import stuff\n",
    "#     def wrapper_func(*args, **kwargs):\n",
    "#         do_more_things\n",
    "#         return original(*args, **kwargs)\n",
    "#     return wrapper\n",
    "\n",
    "\n",
    "# display = wrapper(display)\n",
    "# function_pointer = display\n",
    "# function_trigger = display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd5497b91011d696502c11e3f599a4f8cd017170ff509027ce525b14ed758dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
